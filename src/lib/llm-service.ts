import OpenAI from 'openai';

// LLMæœåŠ¡å°è£…ç±»
// æ³¨æ„ï¼šæ¨¡å‹é€‰æ‹©ç”±æœåŠ¡å™¨ç«¯ç¡¬ç¼–ç æ§åˆ¶ï¼Œå‰ç«¯ä¸ä¼ é€’modelå‚æ•°
// èŠå¤©åŠŸèƒ½ä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨ï¼ŒembeddingåŠŸèƒ½ä½¿ç”¨OpenAI
export class LLMService {
  private baseUrl: string;
  private apiKey?: string;
  private openai: OpenAI;

  constructor() {
    // æ”¯æŒç¯å¢ƒå˜é‡é…ç½®æœåŠ¡å™¨åœ°å€ï¼Œæ–¹ä¾¿å¼€å‘å’Œéƒ¨ç½²
    this.baseUrl = process.env.LLM_SERVER_URL || 'http://localhost:8000';
    this.apiKey = process.env.CHAT_BOT_API_KEY;
    
    // è°ƒè¯•ä¿¡æ¯ - å¸®åŠ©è¯Šæ–­Verceléƒ¨ç½²é—®é¢˜
    console.log('ğŸ”§ LLM Service åˆå§‹åŒ–:');
    console.log('  - ç¯å¢ƒ:', process.env.NODE_ENV);
    console.log('  - æœåŠ¡å™¨URL:', this.baseUrl);
    console.log('  - APIå¯†é’¥å­˜åœ¨:', !!this.apiKey);
    console.log('  - APIå¯†é’¥é•¿åº¦:', this.apiKey?.length || 0);
    console.log('  - OPENAIå¯†é’¥å­˜åœ¨:', !!process.env.OPENAI_API_KEY);
    
    // åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ç”¨äºembedding
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || '',
    });
  }

  // èŠå¤©è¡¥å…¨
  async chat(prompt: string, options: {
    maxTokens?: number;
    temperature?: number;
  } = {}) {
    try {
      const {
        maxTokens = 2000,
        temperature = 0.7
      } = options;

      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
      };

      if (this.apiKey) {
        headers['Authorization'] = `Bearer ${this.apiKey}`;
      }

      const requestBody = {
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: maxTokens,
        temperature,
      };

      const response = await fetch(`${this.baseUrl}/api/v1/chat`, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();

      return {
        success: true,
        content: data.choices[0]?.message?.content || '',
        usage: data.usage,
        model: data.model
      };
    } catch (error) {
      console.error('LLM service error:', error);
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
        content: ''
      };
    }
  }

  // åŸºäºæ¶ˆæ¯å†å²çš„å¯¹è¯
  async chatWithHistory(messages: Array<{role: string, content: string}>, options: {
    maxTokens?: number;
    temperature?: number;
  } = {}) {
    console.log('ğŸš€ å¼€å§‹LLMè°ƒç”¨:', {
      baseUrl: this.baseUrl,
      hasApiKey: !!this.apiKey,
      messagesCount: messages.length,
      options
    });

    try {
      const {
        maxTokens = 2000,
        temperature = 0.7
      } = options;

      // è¿‡æ»¤æ‰ä¸æ”¯æŒçš„è§’è‰²ç±»å‹ï¼Œåªä¿ç•™ 'user', 'assistant', 'system'
      const validMessages = messages.filter(msg => 
        ['user', 'assistant', 'system'].includes(msg.role)
      ).map(msg => ({
        role: msg.role as 'user' | 'assistant' | 'system',
        content: msg.content
      }));

      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
      };

      if (this.apiKey) {
        headers['Authorization'] = `Bearer ${this.apiKey}`;
      }

      const requestBody = {
        messages: validMessages,
        max_tokens: maxTokens,
        temperature,
      };

      console.log('ğŸ“¤ å‘é€è¯·æ±‚åˆ°:', `${this.baseUrl}/api/v1/chat`);
      console.log('ğŸ“¤ è¯·æ±‚å¤´:', { ...headers, Authorization: this.apiKey ? 'Bearer [HIDDEN]' : undefined });
      console.log('ğŸ“¤ è¯·æ±‚ä½“:', { ...requestBody, messages: `${validMessages.length} messages` });

      const response = await fetch(`${this.baseUrl}/api/v1/chat`, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody),
      });

      console.log('ğŸ“¥ å“åº”çŠ¶æ€:', response.status, response.statusText);

      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ APIé”™è¯¯å“åº”:', errorText);
        throw new Error(`HTTP ${response.status}: ${response.statusText} - ${errorText}`);
      }

      const data = await response.json();
      console.log('âœ… APIè°ƒç”¨æˆåŠŸ');

      return {
        success: true,
        content: data.choices[0]?.message?.content || '',
        usage: data.usage,
        model: data.model
      };
    } catch (error) {
      console.error('âŒ LLM service error:', error);
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
        content: ''
      };
    }
  }

  // ç”Ÿæˆæ–‡æœ¬embedding (ä½¿ç”¨OpenAI API)
  async generateEmbedding(text: string): Promise<number[]> {
    try {
      const response = await this.openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: text,
      });
      
      return response.data[0].embedding;
    } catch (error) {
      console.error('Embedding generation error:', error);
      return [];
    }
  }
}

// å•ä¾‹å®ä¾‹
export const llmService = new LLMService();

// é»˜è®¤è®¾ç½®
export const DEFAULT_LLM_SETTINGS = {
  messageMergeSeconds: 300,      // 5åˆ†é’Ÿ
  topicTriggerHours: 24,         // 24å°æ—¶
  maxChatHistory: 10,            // æœ€å¤š10æ¡å†å²
  knowledgeSearchLimit: 5,       // æœ€å¤š5æ¡çŸ¥è¯†åº“ç»“æœ
  llmMaxTokens: 2000,           // LLMæœ€å¤§Tokenæ•°
  llmTemperature: 0.7           // LLMæ¸©åº¦å‚æ•°
}; 